{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNg1CEf8qRTJeDh4AQj+Ri",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamsungmin/DataTrainAnalysis/blob/main/project_week14_%EB%8D%B0%EC%9D%B4%ED%84%B0%ED%99%9C%EC%9A%A9%ED%95%9C_AI_%EC%B1%97%EB%B4%87_%EC%83%9D%EC%84%B1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. 데이터 전처리 및 청크 생성**\n",
        "\n",
        "<aside>\n",
        "\n",
        "**진행 프로세스**\n",
        "\n",
        "스마트폰 제조공정에서는 각 단계별 품질기준, 검사 절차, 불량 대응 방안 등이 포함된 품질관리 매뉴얼 문서(HTML) 를 운영 중입니다.\n",
        "이 문서는 향후 품질분석 GPT 챗봇의 핵심 학습 데이터로 활용되며, 이를 위해 HTML 문서를 자동으로 섹션 단위로 **구조화(청크화)** 해야 합니다.\n",
        "\n",
        "- report.html 파일을 불러와 **<section> 단위로 내용을 분석**하고, 각 섹션을 하나의 **품질매뉴얼 청크(Chunk)를 구성**합니다.  (청크구성은 최종결과물에 따라 품질이 결정됨으로 정답은 없음)\n",
        "    - 각 섹션의 id와 data-section 속성을 함께 추출하여 고유 식별자로 활용\n",
        "    - 청크별 텍스트를 [섹션명] 내용 형식으로 구성하여 구조화\n",
        "    - chunks와 chunk_ids를 딕셔너리 형태로 저장하여 다음 단계에서 활용 가능하도록 구성\n",
        "- 추출된 청크 데이터를 검증하고, 결과를 chunks_data.pkl 파일로 저장합니다.\n",
        "    - 전체 청크 개수와 일부 예시를 출력하여 추출 결과 확인\n",
        "</aside>"
      ],
      "metadata": {
        "id": "qKKgrwAb0EKR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUHawNPMz6yH",
        "outputId": "be139740-921c-4a9d-e6b0-0c533f2711dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 추출된 청크: 22\n",
            "추출된 청크 예시\n",
            "- [개요] 1. 개요1.1 목적본 매뉴얼은ABC전자 스마트폰 제조공정에서의 품질관리 기준과 절차를 정의하여고객 만족도 향상과불량률 최소화를 목표로 합니다.1.2 적용범위스마트폰 조립라인 전체 공정부품 입고검사부터 완제품 출하까지모든 품질관리 담당자 및 생산직 근로자...\n",
            "- [목적] 1.1 목적본 매뉴얼은ABC전자 스마트폰 제조공정에서의 품질관리 기준과 절차를 정의하여고객 만족도 향상과불량률 최소화를 목표로 합니다....\n",
            "- [적용범위] 1.2 적용범위스마트폰 조립라인 전체 공정부품 입고검사부터 완제품 출하까지모든 품질관리 담당자 및 생산직 근로자...\n",
            "- [주요 공정별 품질 기준] 2. 주요 공정별 품질 기준2.1 부품 입고검사 (IQC)검사항목디스플레이 패널: 화소 불량0.01% 이하배터리: 용량 편차±3% 이내카메라 모듈: 해상도 테스트100% 합격메인보드:전기적 특성 검사 필수불량 처리불량률5% 초과 시 전량 반품공급업체 개선요구서 발행2.2 조립공정 (Assembly)주요 체크포인트디스플레이 부착접착...\n",
            "- [부품 입고검사 (IQC)] 2.1 부품 입고검사 (IQC)검사항목디스플레이 패널: 화소 불량0.01% 이하배터리: 용량 편차±3% 이내카메라 모듈: 해상도 테스트100% 합격메인보드:전기적 특성 검사 필수불량 처리불량률5% 초과 시 전량 반품공급업체 개선요구서 발행...\n",
            "청크 데이터 저장 완료 to chunks_data.pkl\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pickle\n",
        "\n",
        "with open('/content/report.html', 'r', encoding='utf-8') as f:\n",
        "    html_content = f.read()\n",
        "\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "chunks = []\n",
        "chunk_ids = []\n",
        "\n",
        "for section in soup.find_all(attrs={'data-section': True}):\n",
        "    section_id = section.get('id')\n",
        "    section_name = section.get('data-section')\n",
        "    section_text = section.get_text(strip=True)\n",
        "\n",
        "    chunk = f\"[{section_name}] {section_text}\"\n",
        "\n",
        "    chunks.append(chunk)\n",
        "    chunk_ids.append(section_id)\n",
        "\n",
        "chunks_data = {\n",
        "    'chunks': chunks,\n",
        "    'chunk_ids': chunk_ids\n",
        "}\n",
        "\n",
        "print(f\"총 추출된 청크: {len(chunks_data['chunks'])}\")\n",
        "print(\"추출된 청크 예시\")\n",
        "for i in range(min(5, len(chunks_data['chunks']))):\n",
        "    print(f\"- {chunks_data['chunks'][i][:200]}...\") # Print first 200 characters of the chunk\n",
        "\n",
        "with open('chunks_data.pkl', 'wb') as f:\n",
        "    pickle.dump(chunks_data, f)\n",
        "\n",
        "print(\"청크 데이터 저장 완료 to chunks_data.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. 임베딩생성**\n",
        "\n",
        "<aside>\n",
        "\n",
        "**진행 프로세스**\n",
        "\n",
        "스마트폰 제조공정에서 사용되는 품질관리 매뉴얼 청크 데이터(chunks_data.pkl) 는 이미 섹션 단위로 구조화된 텍스트지만, 이 상태로는 GPT 모델이 각 청크의 의미적 유사성을 이해하거나 검색할 수 없습니다.  **따라서, 각 청크를 벡터(Embedding) 형태로 변환해 의미를 수치화해야 합니다.**\n",
        "이를 통해 문장 간의 유사도를 계산하고, 사용자가 질문했을 때 가장 관련성 높은 청크를 효율적으로 검색할 수 있습니다.\n",
        "\n",
        "- `chunks_data.pkl` chunks_data.pkl파일을 불러와 각 청크의 텍스트를 **의미 벡터(Embedding)** 로 변환하는 프로그램을 작성합니다.\n",
        "    - OpenAI API를 이용하여 텍스트를 임베딩 벡터로 변환\n",
        "    - 적절한 임베딩 모델(`text-embedding-3-small` 등)을 선택하여 벡터 생성\n",
        "    - 각 청크의 ID와 임베딩 결과를 매핑하여 구조적으로 관리\n",
        "- 생성된 임베딩 벡터를 검증하고, 결과를 `embeddings_data.pkl` 파일로 저장합니다.\n",
        "    - 전체 청크 개수, 벡터 차원, 배열 크기 등을 출력하여 정상 변환 여부 확인\n",
        "    - 일부 샘플 벡터(예: 첫 번째 청크의 앞 10개 값)를 출력해 임베딩 결과를 검증\n",
        "    - `embeddings`, `chunks`, `chunk_ids`를 포함하는 딕셔너리 형태로 저장\n",
        "</aside>"
      ],
      "metadata": {
        "id": "T2gFfOsB1dwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import numpy as np\n",
        "from google.colab import userdata\n",
        "\n",
        "with open('chunks_data.pkl', 'rb') as f:\n",
        "    chunks_data = pickle.load(f)\n",
        "\n",
        "chunks = chunks_data['chunks']\n",
        "chunk_ids = chunks_data['chunk_ids']\n",
        "\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "if openai_api_key is None:\n",
        "    raise ValueError(\"OPENAI_API_KEY not found in Colab Secrets. Please add it.\")\n",
        "\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
        "   text = text.replace(\"\\n\", \" \")\n",
        "   return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
        "\n",
        "embeddings = []\n",
        "total_chunks = len(chunks)\n",
        "print(\"임베딩 생성 중...\")\n",
        "for i, chunk in enumerate(chunks):\n",
        "    embedding = get_embedding(chunk)\n",
        "    embeddings.append(embedding)\n",
        "    print(f\"처리 중 : {i+1}/{total_chunks}\")\n",
        "\n",
        "embeddings_np = np.array(embeddings)\n",
        "\n",
        "embeddings_data = {\n",
        "    'embeddings': embeddings_np,\n",
        "    'chunks': chunks,\n",
        "    'chunk_ids': chunk_ids\n",
        "}\n",
        "\n",
        "print(f\"청크 개수: {len(embeddings_data['embeddings'])}\")\n",
        "print(f\"벡터 차원: {embeddings_data['embeddings'].shape[1]}\")\n",
        "print(f\"배열 크기: {embeddings_data['embeddings'].shape}\")\n",
        "print(\"벡터 샘플 처음 10개:\")\n",
        "print(embeddings_data['embeddings'][0][:10])\n",
        "\n",
        "with open('embeddings_data.pkl', 'wb') as f:\n",
        "    pickle.dump(embeddings_data, f)\n",
        "\n",
        "print(\"임베딩 데이터 저장 완료 : embeddings_data.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecksaFuL1hli",
        "outputId": "a90fc712-44ad-4ae4-fbac-7de28ea419c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "임베딩 생성 중...\n",
            "처리 중 : 1/22\n",
            "처리 중 : 2/22\n",
            "처리 중 : 3/22\n",
            "처리 중 : 4/22\n",
            "처리 중 : 5/22\n",
            "처리 중 : 6/22\n",
            "처리 중 : 7/22\n",
            "처리 중 : 8/22\n",
            "처리 중 : 9/22\n",
            "처리 중 : 10/22\n",
            "처리 중 : 11/22\n",
            "처리 중 : 12/22\n",
            "처리 중 : 13/22\n",
            "처리 중 : 14/22\n",
            "처리 중 : 15/22\n",
            "처리 중 : 16/22\n",
            "처리 중 : 17/22\n",
            "처리 중 : 18/22\n",
            "처리 중 : 19/22\n",
            "처리 중 : 20/22\n",
            "처리 중 : 21/22\n",
            "처리 중 : 22/22\n",
            "청크 개수: 22\n",
            "벡터 차원: 1536\n",
            "배열 크기: (22, 1536)\n",
            "벡터 샘플 처음 10개:\n",
            "[ 0.00648111  0.06691471 -0.01022252  0.00642101 -0.00710218 -0.01945335\n",
            "  0.02155695 -0.00035217 -0.01017244 -0.00572982]\n",
            "임베딩 데이터 저장 완료 : embeddings_data.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.  FAISS 인덱스 생성**\n",
        "\n",
        "<aside>\n",
        "\n",
        "**진행 프로세스**\n",
        "\n",
        "생성된 청크 데이터는 이미 텍스트 임베딩 과정을 통해 의미 벡터로 변환되어 있지만,  수천 개 이상의 벡터를 직접 비교하여 검색하는 것은 비효율적이므로, 벡터데이터베이스인 **FAISS**(Facebook AI Similarity Search) 를 이용해 **빠르고 정확한 유사도 검색 인덱**스를 **구축**해야 합니다.\n",
        "\n",
        "- FAISS 라이브러리를 이용해 인덱스를 생성합니다. (인덱스 종류는 선택함)\n",
        "    - 대량의 임베딩 데이터를 **고속 검색 가능한 벡터 인덱스**로 변환\n",
        "    - L2 거리 기반으로 **의미적 유사 청크를 탐색**할 수 있는 구조 설계\n",
        "    - 인덱스의 검색 결과를 검증하고, 향후 **RAG 질의응답 시스템의 핵심 검색 모듈**로 활용\n",
        "- **인덱스, 임베딩, 청크, 청크 ID**를 포함한 객체를 저장합니다.\n",
        "    - `rag_system.pkl` 파일로 저장\n",
        "</aside>"
      ],
      "metadata": {
        "id": "hIrK-co36xj9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ef8f877",
        "outputId": "f8aeba0a-a84f-4d2c-acea-36b8c4135000"
      },
      "source": [
        "!pip install faiss-cpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2e38ed1",
        "outputId": "120aa151-4a03-4db6-f978-4b07511aa18f"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import faiss\n",
        "import os\n",
        "\n",
        "with open('embeddings_data.pkl', 'rb') as f:\n",
        "    embeddings_data = pickle.load(f)\n",
        "\n",
        "embeddings = embeddings_data['embeddings']\n",
        "chunks = embeddings_data['chunks']\n",
        "chunk_ids = embeddings_data['chunk_ids']\n",
        "\n",
        "embeddings = embeddings.astype('float32')\n",
        "\n",
        "dimension = embeddings.shape[1]\n",
        "\n",
        "print(f\"데이터 로드 완료.\")\n",
        "print(f\"벡터 개수: {len(embeddings)}\")\n",
        "print(f\"벡터 차원: {dimension}\")\n",
        "\n",
        "\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "index.add(embeddings)\n",
        "\n",
        "print(f\"FAISS 인덱스 생성 완료!\")\n",
        "print(f\"인덱스 타입: {type(index)}\")\n",
        "print(f\"저장된 백터 개수 : {index.ntotal} \")\n",
        "\n",
        "\n",
        "rag_system_data = {\n",
        "    'index': index,\n",
        "    'embeddings': embeddings,\n",
        "    'chunks': chunks,\n",
        "    'chunk_ids': chunk_ids\n",
        "}\n",
        "\n",
        "with open('rag_system.pkl', 'wb') as f:\n",
        "    pickle.dump(rag_system_data, f)\n",
        "\n",
        "print(\"RAG 시스템 저장 완료 : rag_system.pkl\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"첫 번째 청크와 가장 유사한 상위 3개 청크 검색 결과\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "query_embedding = embeddings[0].reshape(1, -1).astype('float32')\n",
        "\n",
        "k = 4\n",
        "distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "print(f\"쿼리 청크 ID: {chunk_ids[0]}\")\n",
        "print(f\"쿼리 청크 (처음 200자): {chunks[0][:200]}...\")\n",
        "print(\"-\" * 50)\n",
        "print(\"유사도 검색 결과 (상위 3개):\")\n",
        "\n",
        "for i in range(1, k):\n",
        "    similar_chunk_index = indices[0][i]\n",
        "    similar_chunk_distance = distances[0][i]\n",
        "    similar_chunk = chunks[similar_chunk_index]\n",
        "    similar_chunk_id = chunk_ids[similar_chunk_index]\n",
        "\n",
        "    print(f\"{i}위:\")\n",
        "    print(f\"  청크 ID: {similar_chunk_id}\")\n",
        "    print(f\"  청크 (처음 200자): {similar_chunk[:200]}...\")\n",
        "    print(f\"  유사도 거리 (L2): {similar_chunk_distance}\")\n",
        "    print(\"-\" * 20)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 로드 완료.\n",
            "벡터 개수: 22\n",
            "벡터 차원: 1536\n",
            "FAISS 인덱스 생성 완료!\n",
            "인덱스 타입: <class 'faiss.swigfaiss_avx2.IndexFlatL2'>\n",
            "저장된 백터 개수 : 22 \n",
            "RAG 시스템 저장 완료 : rag_system.pkl\n",
            "\n",
            "==================================================\n",
            "첫 번째 청크와 가장 유사한 상위 3개 청크 검색 결과\n",
            "==================================================\n",
            "쿼리 청크 ID: sec-1\n",
            "쿼리 청크 (처음 200자): [개요] 1. 개요1.1 목적본 매뉴얼은ABC전자 스마트폰 제조공정에서의 품질관리 기준과 절차를 정의하여고객 만족도 향상과불량률 최소화를 목표로 합니다.1.2 적용범위스마트폰 조립라인 전체 공정부품 입고검사부터 완제품 출하까지모든 품질관리 담당자 및 생산직 근로자...\n",
            "--------------------------------------------------\n",
            "유사도 검색 결과 (상위 3개):\n",
            "1위:\n",
            "  청크 ID: sec-1-1\n",
            "  청크 (처음 200자): [목적] 1.1 목적본 매뉴얼은ABC전자 스마트폰 제조공정에서의 품질관리 기준과 절차를 정의하여고객 만족도 향상과불량률 최소화를 목표로 합니다....\n",
            "  유사도 거리 (L2): 0.2851194441318512\n",
            "--------------------\n",
            "2위:\n",
            "  청크 ID: sec-1-2\n",
            "  청크 (처음 200자): [적용범위] 1.2 적용범위스마트폰 조립라인 전체 공정부품 입고검사부터 완제품 출하까지모든 품질관리 담당자 및 생산직 근로자...\n",
            "  유사도 거리 (L2): 0.7820435762405396\n",
            "--------------------\n",
            "3위:\n",
            "  청크 ID: sec-2\n",
            "  청크 (처음 200자): [주요 공정별 품질 기준] 2. 주요 공정별 품질 기준2.1 부품 입고검사 (IQC)검사항목디스플레이 패널: 화소 불량0.01% 이하배터리: 용량 편차±3% 이내카메라 모듈: 해상도 테스트100% 합격메인보드:전기적 특성 검사 필수불량 처리불량률5% 초과 시 전량 반품공급업체 개선요구서 발행2.2 조립공정 (Assembly)주요 체크포인트디스플레이 부착접착...\n",
            "  유사도 거리 (L2): 1.03091299533844\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4 .질문검색 및 답변생성**\n",
        "\n",
        "<aside>\n",
        "\n",
        "**진행 프로세스**\n",
        "\n",
        "축적된 **품질관리 매뉴얼 데이터**를 효율적으로 활용하기 위해서는, 사용자가 질의한 내용을 문서 내에서 **의미적으로 검색하고, GPT를 통해 정확한 답변을 생성**할 수 있어야 합니다.\n",
        "\n",
        "이단계는 RAG 시스템의 **최종 단계로서**, 앞서 구축된 **FAISS 인덱스와 임베딩 데이터**를 이용하여 다음을 달성하는 것을 목표로 합니다:\n",
        "\n",
        "- **사용자의 질문을 임베딩 벡터로 변환하여** 단순 키워드 검색이 아닌, **의미 기반 검색(semantic retrieval)** 을 진행합니다.\n",
        "    - 사용자의 자연어 질문을 OpenAI 임베딩 모델을 통해 벡터로 변환\n",
        "    - 생성된 질문 벡터를 활용해 의미적으로 유사한 문서를 검색할 수 있도록 준비\n",
        "- **FAISS 인덱스를 활용해 관련 문서를 탐색하고**  **품질관리 GPT 챗봇의 실질적 질의응답 기능을 완성하여 GPT로 답변을 생성합니다.**\n",
        "    - 변환된 질문 벡터를 FAISS 인덱스에 입력하여 **가장 유사한 문서 청크**를 효율적으로 탐색\n",
        "    - 검색된 문서들을 GPT의 **컨텍스트(context)** 로 제공하여, 품질관리 전문가 수준의 답변을 자동 생성\n",
        "</aside>"
      ],
      "metadata": {
        "id": "bOFqlYEu9Unp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import faiss\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "import textwrap\n",
        "\n",
        "# 저장된 RAG 시스템 데이터 불러오기\n",
        "try:\n",
        "    with open('rag_system.pkl', 'rb') as f:\n",
        "        rag_system_data = pickle.load(f)\n",
        "    index = rag_system_data['index']\n",
        "    embeddings = rag_system_data['embeddings']\n",
        "    chunks = rag_system_data['chunks']\n",
        "    chunk_ids = rag_system_data['chunk_ids']\n",
        "    print(\"RAG 시스템 데이터 불러오기 완료.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"rag_system.pkl 파일을 찾을 수 없습니다. 이전 단계를 먼저 완료해주세요.\")\n",
        "    exit() # 파일이 없으면 스크립트 종료\n",
        "\n",
        "# OpenAI 클라이언트 초기화\n",
        "# Colab Secrets에 'OPENAI_API_KEY'로 API 키를 설정해주세요.\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "if openai_api_key is None:\n",
        "    raise ValueError(\"OPENAI_API_KEY가 Colab Secrets에 설정되지 않았습니다. 추가해주세요.\")\n",
        "\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "# 임베딩 생성 함수 (이전 단계에서 사용한 함수 재사용)\n",
        "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
        "   text = text.replace(\"\\n\", \" \")\n",
        "   # Fix: Access the embedding list from the data attribute and convert to numpy array before reshaping\n",
        "   return np.array(client.embeddings.create(input = [text], model=model).data[0].embedding)\n",
        "\n",
        "# 질의응답 함수 정의\n",
        "def ask_rag(query, index, chunks, chunk_ids, client, k=3):\n",
        "    # 사용자의 질문 임베딩 생성\n",
        "    query_embedding = get_embedding(query).reshape(1, -1).astype('float32')\n",
        "\n",
        "    # FAISS 인덱스에서 유사한 청크 검색\n",
        "    # k+1을 사용하는 이유는 검색 결과의 첫 번째는 항상 자기 자신(질문 임베딩)일 가능성이 높기 때문입니다.\n",
        "    distances, indices = index.search(query_embedding, k + 1)\n",
        "\n",
        "    # 검색된 청크 추출 (자기 자신 제외)\n",
        "    retrieved_chunks = [chunks[i] for i in indices[0] if chunk_ids[indices[0][0]] != chunk_ids[i]][:k]\n",
        "    retrieved_chunk_ids = [chunk_ids[i] for i in indices[0] if chunk_ids[indices[0][0]] != chunk_ids[i]][:k]\n",
        "\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"'{query}' 질문에 대한 검색 결과 (상위 {k}개 청크):\")\n",
        "    print(\"=\"*50)\n",
        "    for i in range(len(retrieved_chunks)):\n",
        "        print(f\"{i+1}위 청크 ID: {retrieved_chunk_ids[i]}\")\n",
        "        print(f\"내용 (처음 200자): {retrieved_chunks[i][:200]}...\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "\n",
        "    # GPT 모델에 전달할 프롬프트 구성\n",
        "    # 검색된 청크를 컨텍스트로 제공합니다.\n",
        "    context = \"\\n\\n\".join(retrieved_chunks)\n",
        "    prompt = f\"다음 정보를 참고하여 질문에 답변하세요:\\n\\n{context}\\n\\n질문: {query}\\n\\n답변:\"\n",
        "\n",
        "    # GPT 모델을 사용하여 답변 생성\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\", # 또는 다른 적절한 모델 선택\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"당신은 품질관리 매뉴얼에 대한 질문에 답변하는 유용한 챗봇입니다.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=500, # 답변 최대 길이 설정\n",
        "            temperature=0.7 # 답변의 다양성 조절\n",
        "        )\n",
        "        answer = response.choices[0].message.content\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"GPT 모델의 답변:\")\n",
        "        print(\"=\"*50)\n",
        "        # 답변 줄바꿈 처리\n",
        "        print(textwrap.fill(answer, width=80))\n",
        "    except Exception as e:\n",
        "        print(f\"\\nGPT 모델 답변 생성 중 오류 발생: {e}\")\n",
        "        answer = \"답변을 생성하지 못했습니다.\"\n",
        "\n",
        "    return answer\n",
        "\n",
        "# 테스트 질문\n",
        "user_query = \"부품 입고 검사 기준이 뭐야?\"\n",
        "\n",
        "# 질의응답 실행\n",
        "rag_response = ask_rag(user_query, index, chunks, chunk_ids, client, k=3)\n",
        "\n",
        "# 다른 질문으로 테스트 가능\n",
        "# user_query = \"최근 고객 불만 사항은 뭐야?\"\n",
        "# rag_response = ask_rag(user_query, index, chunks, chunk_ids, client, k=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJlFbFnG9ZJN",
        "outputId": "6cf2af3d-eff4-4912-d0d2-94dbbeb66d9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG 시스템 데이터 불러오기 완료.\n",
            "\n",
            "==================================================\n",
            "'부품 입고 검사 기준이 뭐야?' 질문에 대한 검색 결과 (상위 3개 청크):\n",
            "==================================================\n",
            "1위 청크 ID: sec-2\n",
            "내용 (처음 200자): [주요 공정별 품질 기준] 2. 주요 공정별 품질 기준2.1 부품 입고검사 (IQC)검사항목디스플레이 패널: 화소 불량0.01% 이하배터리: 용량 편차±3% 이내카메라 모듈: 해상도 테스트100% 합격메인보드:전기적 특성 검사 필수불량 처리불량률5% 초과 시 전량 반품공급업체 개선요구서 발행2.2 조립공정 (Assembly)주요 체크포인트디스플레이 부착접착...\n",
            "--------------------\n",
            "2위 청크 ID: sec-3-2\n",
            "내용 (처음 200자): [불량 대응 절차] 3.2 불량 대응 절차즉시 조치사항불량품 격리 및 태그 부착불량 원인 분석 시작동일 로트 전수검사 실시근본원인 분석5Why 분석법 적용생산조건 재검토공급업체 품질 점검...\n",
            "--------------------\n",
            "3위 청크 ID: sec-7\n",
            "내용 (처음 200자): [교육 및 훈련] 7. 교육 및 훈련7.1 품질교육 프로그램신입사원 교육 (40시간)품질 기본개념불량 식별 방법측정장비 사용법고객 중심 사고기존 직원 보수교육 (분기별 8시간)신규 품질기준 안내불량 사례 분석개선제안 활동7.2 자격증 취득 지원품질관리기사6시그마 벨트ISO 9001 심사원...\n",
            "--------------------\n",
            "\n",
            "==================================================\n",
            "GPT 모델의 답변:\n",
            "==================================================\n",
            "부품 입고 검사 기준은 다음과 같습니다: - 디스플레이 패널: 화소 불량 0.01% 이하 - 배터리: 용량 편차 ±3% 이내 - 카메라 모듈:\n",
            "해상도 테스트 100% 합격 - 메인보드: 전기적 특성 검사 필수 - 불량 처리: 불량률 5% 초과 시 전량 반품 - 공급업체 개선요구서 발행\n"
          ]
        }
      ]
    }
  ]
}
